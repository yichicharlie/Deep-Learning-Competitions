{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 1: Predicting Appropriate Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team name: åé–ƒå‚²oå‰å¸å\n",
    "### members:\n",
    "- 106024510 é™³ä¿Šç©\n",
    "- 106024513 æ›¾å¥•é½Š\n",
    "- 106024519 å³äº¦æŒ¯\n",
    "- 106024509 æ›¾ç«‹è±ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cut word via Jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨çµå·´ä¸­æ–‡åˆ†è©ï¼Œæ¨¡å¼ä½¿ç”¨å…¨æ¨¡å¼ï¼ŒæŠŠå¥ä¸­æ‰€æœ‰çš„å¯ä»¥æˆè©çš„è©èªéƒ½æƒæå‡ºä¾†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# å¥—ä»¶\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "os.chdir('C:/Users/a0972/Desktop/Deep Learning/Competitions')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# è®€æª”\n",
    "NUM_PROGRAM = 8\n",
    "programs = []\n",
    "for i in range(1, NUM_PROGRAM+1):\n",
    "    program = pd.read_csv('Program0%d.csv' % (i))\n",
    "    \n",
    "    print('Program %d' % (i))\n",
    "    print('Episodes: %d' % (len(program)))\n",
    "    print(program.columns)\n",
    "    print()\n",
    "    \n",
    "    print(program.loc[:1]['Content'])\n",
    "    print()\n",
    "    \n",
    "    programs.append(program)\n",
    "    \n",
    "questions = pd.read_csv('Question.csv')\n",
    "print('Question')\n",
    "print('Episodes: %d' % (len(questions)))\n",
    "print(questions.columns)\n",
    "print()\n",
    "print(questions.loc[:2]['Question'])\n",
    "print()\n",
    "\n",
    "for i in range(6):\n",
    "    print(questions.loc[:2]['Option%d' % (i)])\n",
    "    print()\n",
    "    \n",
    "# çµå·´  \n",
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('big5_dict.txt')\n",
    "example_str = 'æˆ‘è¨å­åƒè˜‹æœ'\n",
    "cut_example_str = jieba.lcut(example_str)\n",
    "print(cut_example_str)\n",
    "\n",
    "\n",
    "def jieba_lines(lines): # å°‡è¤‡æ•¸è¡Œåˆ†è©\n",
    "    cut_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        cut_line = jieba.lcut(line, cut_all=True) # å…¨æ¨¡å¼\n",
    "        cut_lines.append(cut_line)\n",
    "    \n",
    "    return cut_lines\n",
    "\n",
    "# åˆ† program\n",
    "cut_programs = []\n",
    "for program in programs:\n",
    "    episodes = len(program)\n",
    "    cut_program = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        lines = program.loc[episode]['Content'].split('\\n')\n",
    "        cut_program.append(jieba_lines(lines))\n",
    "    \n",
    "    cut_programs.append(cut_program)\n",
    "\n",
    "print(\"%d programs\" % len(cut_programs))\n",
    "print(\"%d episodes in program 0\" % len(cut_programs[0]))\n",
    "print(\"%d lines in first episode of program 0\" % len(cut_programs[0][0]))\n",
    "\n",
    "print()\n",
    "print(\"first 3 lines in 1st episode of program 0: \")\n",
    "print(cut_programs[0][0][:3])\n",
    "\n",
    "# åˆ† question\n",
    "cut_questions = []\n",
    "n = len(questions)\n",
    "for i in range(n):\n",
    "    cut_question = []\n",
    "    lines = questions.loc[i]['Question'].split('\\n')\n",
    "    cut_question.append(jieba_lines(lines))\n",
    "    \n",
    "    for j in range(6):\n",
    "        line = questions.loc[i]['Option%d' % (j)]\n",
    "        cut_question.append(jieba.lcut(line, cut_all=True))\n",
    "    \n",
    "    cut_questions.append(cut_question)\n",
    "    \n",
    "    \n",
    "print(\"%d questions\" % len(cut_questions))\n",
    "print(len(cut_questions[0]))\n",
    "\n",
    "# 1 question\n",
    "print(cut_questions[0][0])\n",
    "\n",
    "# 6 optional reponses\n",
    "for i in range(1, 7):\n",
    "    print(cut_questions[0][i])\n",
    "    \n",
    "# å­˜æª”\n",
    "import numpy as np\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç‚ºäº†ä½¿è©åº«æ›´åŠ ä¹¾æ·¨ï¼Œæˆ‘å€‘åˆªé™¤ç©ºç™½ã€æ¨™é»ç¬¦è™Ÿä»¥åŠç©ºå­—ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_blank(x): # ç©ºç™½ã€æ¨™é»ç¬¦è™Ÿ\n",
    "    bad = []\n",
    "    for i in range(0, len(x)) :\n",
    "        if x[i] == ' ' or x[i] == ',' or x[i] == '...' or x[i] == '' :\n",
    "            bad.append(i)\n",
    "    kkk = np.array(bad) - np.array(range(0, len(bad)))\n",
    "    for j in kkk:\n",
    "        del x[j]\n",
    "        \n",
    "    return x\n",
    "\n",
    "def delete_noword(x): # ç©ºå­—ä¸²\n",
    "    bad = []\n",
    "    for i in range(0, len(x)):\n",
    "        if len(x[i]) == 0:\n",
    "            bad.append(i)\n",
    "    kkk = np.array(bad) - np.array(range(0, len(bad)))\n",
    "    for j in kkk :\n",
    "        del x[j]\n",
    "    return x\n",
    "\n",
    "# è™•ç† program    \n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        delete_noword(cut_programs[i][j])\n",
    "\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        for k in range(0, len(cut_programs[i][j])):\n",
    "            cut_programs[i][j][k] = delete_blank(cut_programs[i][j][k]) \n",
    "            \n",
    "# è™•ç† question\n",
    "for i in range(0, 500):\n",
    "    cut_Question[i][0] = delete_blank(list(chain(*cut_Question[i][0])))\n",
    "    for j in range(1, 7):\n",
    "        cut_Question[i][j] = delete_blank(cut_Question[i][j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ©ç”¨ gensim å¥—ä»¶è¨“ç·´ programs çš„è©ï¼Œè½‰æ›æˆ200ç¶­çš„å‘é‡ï¼Œå†å¾ programs è£¡åˆªé™¤ä¸åœ¨è©åº«çš„è©ã€‚\n",
    "\n",
    "åƒæ•¸è¨­å®šï¼š\n",
    "- size=200 \n",
    "- window=10\n",
    "- min_count=50\n",
    "- workers=4 \n",
    "- sg=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# æŠŠè©æ•´ç†å¥½\n",
    "w2v_text = []\n",
    "for i in range(0, 8) :      \n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "       w2v_text = w2v_text + [list(chain(*cut_programs[i][j]))]\n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# è¨“ç·´\n",
    "start = time.time()        \n",
    "        \n",
    "model = Word2Vec(w2v_text,size=200, window=10, min_count=50, workers=4, sg=1)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Running Time : %d min %d sec\"%(math.floor((end - start) / 60),math.ceil((end - start) % 60 )))\n",
    "\n",
    "\n",
    "def in_dict(text, d): # æ˜¯å¦åœ¨å­—å…¸ä¸­\n",
    "    return [x for x in text if x in d]\n",
    "\n",
    "# åˆªé™¤æ²’æœ‰trainåˆ°çš„è©\n",
    "start = time.time()\n",
    "\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        for k in range(0, len(cut_programs[i][j])):\n",
    "            cut_programs[i][j][k] = in_dict(cut_programs[i][j][k], list(model.wv.vocab.keys()))\n",
    "\n",
    "for i in range(0, 500):\n",
    "    cut_Question[i][0] = in_dict(cut_Question[i][0], list(model.wv.vocab.keys()))\n",
    "    for j in range(1, 7):\n",
    "        cut_Question[i][j] = in_dict(cut_Question[i][j], list(model.wv.vocab.keys()))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Running Time : %d min %d sec\"%(math.floor((end - start) / 60),math.ceil((end - start) % 60 )))\n",
    "\n",
    "\n",
    "# å°‡ä¸åœ¨è©åº«çš„è©æ¨æ£„\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        delete_noword(cut_programs[i][j])\n",
    "\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        for k in range(0, len(cut_programs[i][j])):\n",
    "            cut_programs[i][j][k] = delete_blank(cut_programs[i][j][k]) \n",
    "            \n",
    "\n",
    "for i in range(0, 500):\n",
    "    for j in range(0, 7):\n",
    "        cut_Question[i][j] = delete_blank(cut_Question[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training set generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”Ÿæˆ300000ç­†è³‡æ–™ç•¶ä½œ training data ï¼Œæ¯ç­†è³‡æ–™åŒ…å«å…©å€‹å¥å­ï¼Œå…¶ä¸­ç´„è«150000ç­†çš„å¥å­å…·æœ‰å‰å¾Œæ–‡é—œä¿‚ï¼Œå…¶é¤˜å‰‡ç„¡ã€‚\n",
    "æ¥è‘—å°‡å¥ä¸­å–®è©ä½¿ç”¨ word2vec è½‰æ›æˆ200ç¶­çš„å‘é‡ï¼Œå‡è¨­å‰æ–‡èˆ‡å¾Œæ–‡åˆ†åˆ¥æœ‰ç”±å–®è©è½‰æ›è€Œæˆçš„ n å€‹å‘é‡$v_1,...,v_n$èˆ‡ m å€‹å‘é‡$u_1,...,u_m$ï¼Œæˆ‘å€‘è¨­è¨ˆæ¨¡å‹ç‰¹å¾µå¦‚ä¸‹ï¼š\n",
    "\n",
    " 1. average vector 1 : $Vmean=(v_1+\\cdots +v_n)/n$ \n",
    "\n",
    "\n",
    " 2. average vector 2 : $Umean=(u_1+\\cdots +u_m)/m$\n",
    " \n",
    " \n",
    " 3. correlation of average vector 1 and 2 : $Corr=corr(Vmean, Umean)$\n",
    "\n",
    "\n",
    " 4. average maximum correlation every words : $meanmaxCorr = (c_1+\\cdots +c_n)/n, \\quad where\\quad c_i=\\underset{j=1,...,m}{max}corr(v_i,u_j)$\n",
    " \n",
    " \n",
    " å¾—åˆ°300000ç­†402ç¶­çš„training dataã€‚\n",
    " \n",
    " æ¥è‘—å°‡ questions ä¸­çš„ question èˆ‡æ¯å€‹ option é…å°ï¼Œä»¥ç›¸åŒæ–¹å¼åšç‰¹å¾µé¸å–ï¼Œå¾—åˆ°500*6ç­† testing dataã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_TRAIN = 300000\n",
    "TRAIN_VALID_RATE = 0.7\n",
    "NUM_PROGRAM = 8\n",
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "            line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "            \n",
    "            Xs.append([cut_programs[program_id][episode_id][line_id], \n",
    "                       cut_programs[program_id][episode_id][line_id+1]])\n",
    "            Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            first_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            first_episode_id = random.randint(0, len(cut_programs[first_program_id])-1)\n",
    "            first_line_id = random.randint(0, len(cut_programs[first_program_id][first_episode_id])-1)\n",
    "            \n",
    "            second_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            second_episode_id = random.randint(0, len(cut_programs[second_program_id])-1)\n",
    "            second_line_id = random.randint(0, len(cut_programs[second_program_id][second_episode_id])-1)\n",
    "            \n",
    "            Xs.append([cut_programs[first_program_id][first_episode_id][first_line_id], \n",
    "                       cut_programs[second_program_id][second_episode_id][second_line_id]])\n",
    "            Ys.append(0)\n",
    "    \n",
    "    return Xs, Ys\n",
    "\n",
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "# ç‰¹å¾µé¸å–\n",
    "\n",
    "train_X = []\n",
    "\n",
    "for i in range(0, len(Xs)) :\n",
    "    v0 = model[in_dict(Xs[i][0], list(model.wv.vocab.keys()))]\n",
    "    v1 = model[in_dict(Xs[i][1], list(model.wv.vocab.keys()))]\n",
    "    ss = []\n",
    "    for a1 in range(0, v0.shape[0]):\n",
    "        cor_in = []\n",
    "        for a2 in range(0, v1.shape[0]):\n",
    "            cor_in.append(np.absolute(np.corrcoef(v0[a1],v1[a2])[0][1]))\n",
    "        ss.append(np.max(cor_in))\n",
    "    cor = np.mean(ss)\n",
    "\n",
    "    train_X.append(list(np.concatenate((model[Xs[i][0]].mean(axis = 0), model[Xs[i][1]].mean(axis = 0), np.array([abs(cor)])))))\n",
    "    \n",
    "train_X = np.array(train_X)\n",
    "y_train = np.array(Ys)\n",
    "\n",
    "\n",
    "test_X = []\n",
    "\n",
    "for i in range(0, len(cut_Question)):\n",
    "    if len(in_dict(cut_Question[i][0], list(model.wv.vocab.keys()))) == 0:\n",
    "        pass\n",
    "    else :\n",
    "        v0 = model[in_dict(cut_Question[i][0], list(model.wv.vocab.keys()))]\n",
    "        Q = np.mean(v0, axis = 0)\n",
    "        \n",
    "        for j in range(1, 7):\n",
    "            \n",
    "            if len(in_dict(cut_Question[i][j], list(model.wv.vocab.keys()))) == 0:\n",
    "                \n",
    "                cor = 0\n",
    "                \n",
    "            else:\n",
    "                v1 = model[in_dict(cut_Question[i][j], list(model.wv.vocab.keys()))]\n",
    "                A = np.mean(v1, axis = 0)\n",
    "                ss = []\n",
    "                for a1 in range(0, v0.shape[0]):\n",
    "                    cor_in = []\n",
    "                    for a2 in range(0, v1.shape[0]):\n",
    "                        cor_in.append(np.absolute(np.corrcoef(v0[a1],v1[a2])[0][1]))\n",
    "                    ss.append(np.max(cor_in))\n",
    "                cor = np.mean(ss)\n",
    "            \n",
    "            test_X.append(list(np.concatenate((Q, A, np.array([abs(cor)])))))   \n",
    "\n",
    "test_X = np.array(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Model - LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨ LightGBM è¨“ç·´æ¨¡å‹ä¸¦åš 5-fold CVï¼Œå…¶åƒæ•¸ä»¥éå¾€ç¶“é©—å¾®èª¿ã€‚\n",
    "\n",
    "LightGBM æ˜¯ä¸€å€‹æ¢¯åº¦ boosting æ¡†æ¶ï¼Œä½¿ç”¨åŸºæ–¼å­¸ç¿’ç®—æ³•çš„æ±ºç­–æ¨¹ã€‚å®ƒå¯ä»¥èªªæ˜¯åˆ†å¸ƒå¼çš„ï¼Œé«˜æ•ˆçš„ï¼Œæ ¹æ“šå®˜æ–¹æ–‡æª”ä»¥åŠç„¡æ•¸äººçš„å¯¦éš›é«”é©—ï¼Œå®ƒæœ‰ä»¥ä¸‹å„ªå‹¢ï¼š\n",
    "\n",
    "\n",
    "- æ›´å¿«çš„è¨“ç·´æ•ˆç‡\n",
    "\n",
    "- ä½å…§å­˜ä½¿ç”¨\n",
    "\n",
    "- æ›´å¥½çš„æº–ç¢ºç‡\n",
    "\n",
    "- æ”¯æŒä¸¦è¡Œå­¸ç¿’ï¼Œå¯è™•ç†å¤§è¦æ¨¡æ•¸æ“š\n",
    "\n",
    "åœ¨å…¬é–‹æ•¸æ“šä¸Šçš„å¯¦é©—è¡¨æ˜ LightGBM èƒ½åœ¨å­¸ç¿’æ•ˆç‡å’Œæº–ç¢ºç‡ä¸Šéƒ½è¡¨ç¾å‡ºæ¯”å…¶ä»–å·²æœ‰ boosting å·¥å…·æ›´å¥½çš„è¡¨ç¾ï¼Œè€Œä¸”æœ‰è‘—æ›´ä½çš„å…§å­˜æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œå¯¦é©—ä¹Ÿè¡¨æ˜ LightGBM é€šéä½¿ç”¨å¤šå°æ©Ÿå™¨é€²è¡Œç‰¹å®šè¨­å®šçš„è¨“ç·´ï¼Œå®ƒèƒ½å–å¾—ç·šæ€§åŠ é€Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "import lightgbm as lgb \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train = pd.DataFrame(train_X)\n",
    "tauc = np.array(y_train)\n",
    "test = pd.DataFrame(test_X)\n",
    "\n",
    "folds = 5\n",
    "fpred2 = []\n",
    "lgbclf = []\n",
    "\n",
    "kf = KFold(train.shape[0], n_folds=folds)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\n Fold %d\\n' % (i + 1))\n",
    "    X_train, X_val = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train2, y_val2 = tauc[train_index], tauc[test_index]\n",
    "    \n",
    "    # Define cross-validation variables  \n",
    "    param2={\n",
    "    'boosting_type': 'gbdt',\n",
    "    'class_weight': None,\n",
    "    'colsample_bytree': 0.733333,\n",
    "    'learning_rate': 0.0764107,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 460,\n",
    "    'min_child_weight': 0.001,\n",
    "    'min_split_gain': 0.0,\n",
    "    'n_estimators': 7000,\n",
    "    'n_jobs': -1,\n",
    "    'num_leaves': 20,\n",
    "    'objective':'binary',\n",
    "    'random_state': 42,\n",
    "    'reg_alpha': 0.877551,\n",
    "    'reg_lambda': 0.204082,\n",
    "    'silent': True,\n",
    "    'subsample': 0.949495,\n",
    "    'subsample_for_bin': 240000,\n",
    "    'subsample_freq': 1,\n",
    "    'metric': 'auc'\n",
    "    } \n",
    "        \n",
    "    train_data2 = lgb.Dataset(X_train,label=y_train2)\n",
    "    valid_data2 = lgb.Dataset(X_val,label = y_val2)\n",
    "    \n",
    "    #  Build Model\n",
    "    lgbm_clf = lgb.train(param2,\n",
    "    train_data2,\n",
    "    2500,\n",
    "    valid_sets=valid_data2,\n",
    "    early_stopping_rounds= 40,\n",
    "    verbose_eval= 100\n",
    "    )\n",
    "\n",
    "    #  Evaluate Model and Predict  \n",
    "    lgbclf  = lgbclf  + list(lgbm_clf.predict(X_val))\n",
    "    y_pred2 = lgbm_clf.predict(test)\n",
    "\n",
    "    #  feature importance\n",
    "    feature_importances_clf = pd.DataFrame({'feature': train.columns, 'importance': lgbm_clf.feature_importance()})    \n",
    "    zero_importance_features_clf = list(feature_importances_clf.feature[feature_importances_clf.importance == 0])\n",
    "\n",
    "    #  Add Predictions and Average Them\n",
    "    if i > 0:\n",
    "        fpred2 = pred2 + y_pred2\n",
    "        useless = list(set(useless) & set(zero_importance_features_clf) )\n",
    "    else:\n",
    "        fpred2 = y_pred2\n",
    "        useless = zero_importance_features_clf\n",
    "    pred2 = fpred2\n",
    "\n",
    "zero_importance_features_clf_union = useless\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å° Testing data åšé æ¸¬ï¼Œå¾—å‡º6å€‹é¸é …ç‚ºå‰å¾Œæ–‡çš„æ©Ÿç‡å¾Œï¼Œå„åˆ¥ä¹˜ä¸Š $ğ¶ğ‘œğ‘Ÿğ‘Ÿ$ ï¼Œæ“‡å…¶æœ€é«˜ä½œç‚º6å€‹é¸é …ä¸­çš„é æ¸¬çµæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_clf = lgbclf\n",
    "mpred2 = pred2 / folds\n",
    "train = train.drop(labels = zero_importance_features_clf_union, axis = 1)\n",
    "test = test.drop(labels = zero_importance_features_clf_union, axis = 1)\n",
    "\n",
    "train_pred = np.where(np.array(pred_clf) >= 0.5,1,0)\n",
    "test_pred = np.where(mpred2 >= 0.5,1,0)\n",
    "\n",
    "sum(train_pred == tauc)/len(train_pred)\n",
    "\n",
    "accuracy_score(train_pred, tauc)\n",
    "\n",
    "\n",
    "yy = mpred2.reshape(500,6)\n",
    "kk = (np.array(abs(test.iloc[:,400])) * mpred2).reshape(500,6)\n",
    "      \n",
    "prediction = np.argmax(kk,1)\n",
    "\n",
    "submit = pd.read_csv('Sample.csv')\n",
    "       \n",
    "submit['Answer'] =  prediction\n",
    "\n",
    "\n",
    "submit.to_csv('submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions \n",
    "\n",
    "+ Public leaderboardï¼š0.708   \n",
    "+ Private leaderboardï¼š0.656\n",
    "\n",
    "ç”±æ–¼å¹³å¸¸è¼ƒå°‘è™•ç†æ–‡å­—è³‡æ–™çš„ç¶“é©—ï¼Œé€™æ¬¡å¾æ–‡å­—çš„å‰è™•ç†åˆ°å¾ŒçºŒçš„å»ºæ¨¡çš„æ¯ä¸€æ­¥éƒ½éå¸¸å…·æœ‰æŒ‘æˆ°æ€§ï¼Œä½†éç¨‹éå¸¸æœ‰è¶£ï¼Œæˆ‘å€‘å››å€‹äººä¹Ÿå¾ä¸­å­¸åˆ°éå¸¸å¤šã€‚åœ¨æ¯”è³½çš„å‰æœŸæˆ‘å€‘æŠŠé‡å¿ƒæ”¾åœ¨å•å¥èˆ‡å›è¦†ä¹‹é–“correlationçš„è¨ˆç®—ï¼Œæœ‰è¶£çš„æ˜¯é€éword2vecç‚ºåŸºç¤çš„å‘é‡è¨ˆç®—æˆ‘å€‘å°±é”åˆ°äº†60%å·¦å³çš„Accuracyï¼Œæ¯”äº‚çŒœçš„18%é«˜å‡ºä¸å°‘ï¼Œå¾å¯¦ä½œä¸­ç™¼ç¾äº†è¼ƒç›¸é—œçš„å­—è©å‘é‡ä¹‹é–“çš„ç¢ºæœƒæœ‰è¼ƒé«˜çš„correlationã€‚ç„¶è€Œåœ¨å¾ŒæœŸæˆ‘å€‘é€éå»ºmachine learningçš„æ¨¡å‹æ™‚å»è½å…¥äº†public leaderboardçš„é™·é˜±ï¼Œæœ€å¾Œé¸çš„submissionéƒ½æ˜¯åœ¨public leaderboardè¡¨ç¾æ¯”è¼ƒå¥½çš„ï¼Œè€Œä¸æ˜¯åœ¨Cross validationæº–ç¢ºç‡æ¯”è¼ƒé«˜çš„æ¨¡å‹è¨­å®šï¼Œå°è‡´æ²’æœ‰é¸åˆ°private leaderboardè¡¨ç¾æœ€å¥½çš„é‚£å€‹çµæœ(0.68)ã€‚å¾é€™é»æˆ‘å€‘å¾—åˆ°äº†ä¸€å€‹å¯¶è²´çš„ç¶“é©—ï¼Œé‚£å°±æ˜¯å°å¾…validation setçš„çµæœæ‡‰è©²è¦åƒå°å¾…è€å©†é‚£æ¨£ï¼Œè€å©†çµ•å°ä¸æœƒæœ‰éŒ¯ï¼Œå¦‚æœæœ‰éŒ¯é‚£ä¹Ÿä¸€å®šæ˜¯æˆ‘å€‘çœ‹éŒ¯ã€‚æœ€å¾Œï¼ŒçœŸçš„å¾ˆæ„Ÿè¬æœ‰é€™æ¬¡å¾ˆæ£’çš„å¯¦ä½œæ©Ÿæœƒï¼Œä¹Ÿå¸Œæœ›åœ¨æœªä¾†çš„å››å€‹æ¯”è³½æˆ‘å€‘èƒ½å¤ æœ‰ç›¸å°æ›´å¥½çš„çµæœï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
