{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 1: Predicting Appropriate Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team name: 卍閃傲o剎帝卍\n",
    "### members:\n",
    "- 106024510 陳俊穎\n",
    "- 106024513 曾奕齊\n",
    "- 106024519 吳亦振\n",
    "- 106024509 曾立豪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cut word via Jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用結巴中文分詞，模式使用全模式，把句中所有的可以成詞的詞語都掃描出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 套件\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "os.chdir('C:/Users/a0972/Desktop/Deep Learning/Competitions')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 讀檔\n",
    "NUM_PROGRAM = 8\n",
    "programs = []\n",
    "for i in range(1, NUM_PROGRAM+1):\n",
    "    program = pd.read_csv('Program0%d.csv' % (i))\n",
    "    \n",
    "    print('Program %d' % (i))\n",
    "    print('Episodes: %d' % (len(program)))\n",
    "    print(program.columns)\n",
    "    print()\n",
    "    \n",
    "    print(program.loc[:1]['Content'])\n",
    "    print()\n",
    "    \n",
    "    programs.append(program)\n",
    "    \n",
    "questions = pd.read_csv('Question.csv')\n",
    "print('Question')\n",
    "print('Episodes: %d' % (len(questions)))\n",
    "print(questions.columns)\n",
    "print()\n",
    "print(questions.loc[:2]['Question'])\n",
    "print()\n",
    "\n",
    "for i in range(6):\n",
    "    print(questions.loc[:2]['Option%d' % (i)])\n",
    "    print()\n",
    "    \n",
    "# 結巴  \n",
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('big5_dict.txt')\n",
    "example_str = '我討厭吃蘋果'\n",
    "cut_example_str = jieba.lcut(example_str)\n",
    "print(cut_example_str)\n",
    "\n",
    "\n",
    "def jieba_lines(lines): # 將複數行分詞\n",
    "    cut_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        cut_line = jieba.lcut(line, cut_all=True) # 全模式\n",
    "        cut_lines.append(cut_line)\n",
    "    \n",
    "    return cut_lines\n",
    "\n",
    "# 分 program\n",
    "cut_programs = []\n",
    "for program in programs:\n",
    "    episodes = len(program)\n",
    "    cut_program = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        lines = program.loc[episode]['Content'].split('\\n')\n",
    "        cut_program.append(jieba_lines(lines))\n",
    "    \n",
    "    cut_programs.append(cut_program)\n",
    "\n",
    "print(\"%d programs\" % len(cut_programs))\n",
    "print(\"%d episodes in program 0\" % len(cut_programs[0]))\n",
    "print(\"%d lines in first episode of program 0\" % len(cut_programs[0][0]))\n",
    "\n",
    "print()\n",
    "print(\"first 3 lines in 1st episode of program 0: \")\n",
    "print(cut_programs[0][0][:3])\n",
    "\n",
    "# 分 question\n",
    "cut_questions = []\n",
    "n = len(questions)\n",
    "for i in range(n):\n",
    "    cut_question = []\n",
    "    lines = questions.loc[i]['Question'].split('\\n')\n",
    "    cut_question.append(jieba_lines(lines))\n",
    "    \n",
    "    for j in range(6):\n",
    "        line = questions.loc[i]['Option%d' % (j)]\n",
    "        cut_question.append(jieba.lcut(line, cut_all=True))\n",
    "    \n",
    "    cut_questions.append(cut_question)\n",
    "    \n",
    "    \n",
    "print(\"%d questions\" % len(cut_questions))\n",
    "print(len(cut_questions[0]))\n",
    "\n",
    "# 1 question\n",
    "print(cut_questions[0][0])\n",
    "\n",
    "# 6 optional reponses\n",
    "for i in range(1, 7):\n",
    "    print(cut_questions[0][i])\n",
    "    \n",
    "# 存檔\n",
    "import numpy as np\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了使詞庫更加乾淨，我們刪除空白、標點符號以及空字串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_blank(x): # 空白、標點符號\n",
    "    bad = []\n",
    "    for i in range(0, len(x)) :\n",
    "        if x[i] == ' ' or x[i] == ',' or x[i] == '...' or x[i] == '' :\n",
    "            bad.append(i)\n",
    "    kkk = np.array(bad) - np.array(range(0, len(bad)))\n",
    "    for j in kkk:\n",
    "        del x[j]\n",
    "        \n",
    "    return x\n",
    "\n",
    "def delete_noword(x): # 空字串\n",
    "    bad = []\n",
    "    for i in range(0, len(x)):\n",
    "        if len(x[i]) == 0:\n",
    "            bad.append(i)\n",
    "    kkk = np.array(bad) - np.array(range(0, len(bad)))\n",
    "    for j in kkk :\n",
    "        del x[j]\n",
    "    return x\n",
    "\n",
    "# 處理 program    \n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        delete_noword(cut_programs[i][j])\n",
    "\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        for k in range(0, len(cut_programs[i][j])):\n",
    "            cut_programs[i][j][k] = delete_blank(cut_programs[i][j][k]) \n",
    "            \n",
    "# 處理 question\n",
    "for i in range(0, 500):\n",
    "    cut_Question[i][0] = delete_blank(list(chain(*cut_Question[i][0])))\n",
    "    for j in range(1, 7):\n",
    "        cut_Question[i][j] = delete_blank(cut_Question[i][j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 gensim 套件訓練 programs 的詞，轉換成200維的向量，再從 programs 裡刪除不在詞庫的詞。\n",
    "\n",
    "參數設定：\n",
    "- size=200 \n",
    "- window=10\n",
    "- min_count=50\n",
    "- workers=4 \n",
    "- sg=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把詞整理好\n",
    "w2v_text = []\n",
    "for i in range(0, 8) :      \n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "       w2v_text = w2v_text + [list(chain(*cut_programs[i][j]))]\n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# 訓練\n",
    "start = time.time()        \n",
    "        \n",
    "model = Word2Vec(w2v_text,size=200, window=10, min_count=50, workers=4, sg=1)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Running Time : %d min %d sec\"%(math.floor((end - start) / 60),math.ceil((end - start) % 60 )))\n",
    "\n",
    "\n",
    "def in_dict(text, d): # 是否在字典中\n",
    "    return [x for x in text if x in d]\n",
    "\n",
    "# 刪除沒有train到的詞\n",
    "start = time.time()\n",
    "\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        for k in range(0, len(cut_programs[i][j])):\n",
    "            cut_programs[i][j][k] = in_dict(cut_programs[i][j][k], list(model.wv.vocab.keys()))\n",
    "\n",
    "for i in range(0, 500):\n",
    "    cut_Question[i][0] = in_dict(cut_Question[i][0], list(model.wv.vocab.keys()))\n",
    "    for j in range(1, 7):\n",
    "        cut_Question[i][j] = in_dict(cut_Question[i][j], list(model.wv.vocab.keys()))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Running Time : %d min %d sec\"%(math.floor((end - start) / 60),math.ceil((end - start) % 60 )))\n",
    "\n",
    "\n",
    "# 將不在詞庫的詞捨棄\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        delete_noword(cut_programs[i][j])\n",
    "\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, len(cut_programs[i])) :\n",
    "        for k in range(0, len(cut_programs[i][j])):\n",
    "            cut_programs[i][j][k] = delete_blank(cut_programs[i][j][k]) \n",
    "            \n",
    "\n",
    "for i in range(0, 500):\n",
    "    for j in range(0, 7):\n",
    "        cut_Question[i][j] = delete_blank(cut_Question[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training set generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成300000筆資料當作 training data ，每筆資料包含兩個句子，其中約莫150000筆的句子具有前後文關係，其餘則無。\n",
    "接著將句中單詞使用 word2vec 轉換成200維的向量，假設前文與後文分別有由單詞轉換而成的 n 個向量$v_1,...,v_n$與 m 個向量$u_1,...,u_m$，我們設計模型特徵如下：\n",
    "\n",
    " 1. average vector 1 : $Vmean=(v_1+\\cdots +v_n)/n$ \n",
    "\n",
    "\n",
    " 2. average vector 2 : $Umean=(u_1+\\cdots +u_m)/m$\n",
    " \n",
    " \n",
    " 3. correlation of average vector 1 and 2 : $Corr=corr(Vmean, Umean)$\n",
    "\n",
    "\n",
    " 4. average maximum correlation every words : $meanmaxCorr = (c_1+\\cdots +c_n)/n, \\quad where\\quad c_i=\\underset{j=1,...,m}{max}corr(v_i,u_j)$\n",
    " \n",
    " \n",
    " 得到300000筆402維的training data。\n",
    " \n",
    " 接著將 questions 中的 question 與每個 option 配對，以相同方式做特徵選取，得到500*6筆 testing data。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_TRAIN = 300000\n",
    "TRAIN_VALID_RATE = 0.7\n",
    "NUM_PROGRAM = 8\n",
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "            line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "            \n",
    "            Xs.append([cut_programs[program_id][episode_id][line_id], \n",
    "                       cut_programs[program_id][episode_id][line_id+1]])\n",
    "            Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            first_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            first_episode_id = random.randint(0, len(cut_programs[first_program_id])-1)\n",
    "            first_line_id = random.randint(0, len(cut_programs[first_program_id][first_episode_id])-1)\n",
    "            \n",
    "            second_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            second_episode_id = random.randint(0, len(cut_programs[second_program_id])-1)\n",
    "            second_line_id = random.randint(0, len(cut_programs[second_program_id][second_episode_id])-1)\n",
    "            \n",
    "            Xs.append([cut_programs[first_program_id][first_episode_id][first_line_id], \n",
    "                       cut_programs[second_program_id][second_episode_id][second_line_id]])\n",
    "            Ys.append(0)\n",
    "    \n",
    "    return Xs, Ys\n",
    "\n",
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "# 特徵選取\n",
    "\n",
    "train_X = []\n",
    "\n",
    "for i in range(0, len(Xs)) :\n",
    "    v0 = model[in_dict(Xs[i][0], list(model.wv.vocab.keys()))]\n",
    "    v1 = model[in_dict(Xs[i][1], list(model.wv.vocab.keys()))]\n",
    "    ss = []\n",
    "    for a1 in range(0, v0.shape[0]):\n",
    "        cor_in = []\n",
    "        for a2 in range(0, v1.shape[0]):\n",
    "            cor_in.append(np.absolute(np.corrcoef(v0[a1],v1[a2])[0][1]))\n",
    "        ss.append(np.max(cor_in))\n",
    "    cor = np.mean(ss)\n",
    "\n",
    "    train_X.append(list(np.concatenate((model[Xs[i][0]].mean(axis = 0), model[Xs[i][1]].mean(axis = 0), np.array([abs(cor)])))))\n",
    "    \n",
    "train_X = np.array(train_X)\n",
    "y_train = np.array(Ys)\n",
    "\n",
    "\n",
    "test_X = []\n",
    "\n",
    "for i in range(0, len(cut_Question)):\n",
    "    if len(in_dict(cut_Question[i][0], list(model.wv.vocab.keys()))) == 0:\n",
    "        pass\n",
    "    else :\n",
    "        v0 = model[in_dict(cut_Question[i][0], list(model.wv.vocab.keys()))]\n",
    "        Q = np.mean(v0, axis = 0)\n",
    "        \n",
    "        for j in range(1, 7):\n",
    "            \n",
    "            if len(in_dict(cut_Question[i][j], list(model.wv.vocab.keys()))) == 0:\n",
    "                \n",
    "                cor = 0\n",
    "                \n",
    "            else:\n",
    "                v1 = model[in_dict(cut_Question[i][j], list(model.wv.vocab.keys()))]\n",
    "                A = np.mean(v1, axis = 0)\n",
    "                ss = []\n",
    "                for a1 in range(0, v0.shape[0]):\n",
    "                    cor_in = []\n",
    "                    for a2 in range(0, v1.shape[0]):\n",
    "                        cor_in.append(np.absolute(np.corrcoef(v0[a1],v1[a2])[0][1]))\n",
    "                    ss.append(np.max(cor_in))\n",
    "                cor = np.mean(ss)\n",
    "            \n",
    "            test_X.append(list(np.concatenate((Q, A, np.array([abs(cor)])))))   \n",
    "\n",
    "test_X = np.array(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Model - LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 LightGBM 訓練模型並做 5-fold CV，其參數以過往經驗微調。\n",
    "\n",
    "LightGBM 是一個梯度 boosting 框架，使用基於學習算法的決策樹。它可以說是分布式的，高效的，根據官方文檔以及無數人的實際體驗，它有以下優勢：\n",
    "\n",
    "\n",
    "- 更快的訓練效率\n",
    "\n",
    "- 低內存使用\n",
    "\n",
    "- 更好的準確率\n",
    "\n",
    "- 支持並行學習，可處理大規模數據\n",
    "\n",
    "在公開數據上的實驗表明 LightGBM 能在學習效率和準確率上都表現出比其他已有 boosting 工具更好的表現，而且有著更低的內存消耗。此外，實驗也表明 LightGBM 通過使用多台機器進行特定設定的訓練，它能取得線性加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "import lightgbm as lgb \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train = pd.DataFrame(train_X)\n",
    "tauc = np.array(y_train)\n",
    "test = pd.DataFrame(test_X)\n",
    "\n",
    "folds = 5\n",
    "fpred2 = []\n",
    "lgbclf = []\n",
    "\n",
    "kf = KFold(train.shape[0], n_folds=folds)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\n Fold %d\\n' % (i + 1))\n",
    "    X_train, X_val = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train2, y_val2 = tauc[train_index], tauc[test_index]\n",
    "    \n",
    "    # Define cross-validation variables  \n",
    "    param2={\n",
    "    'boosting_type': 'gbdt',\n",
    "    'class_weight': None,\n",
    "    'colsample_bytree': 0.733333,\n",
    "    'learning_rate': 0.0764107,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 460,\n",
    "    'min_child_weight': 0.001,\n",
    "    'min_split_gain': 0.0,\n",
    "    'n_estimators': 7000,\n",
    "    'n_jobs': -1,\n",
    "    'num_leaves': 20,\n",
    "    'objective':'binary',\n",
    "    'random_state': 42,\n",
    "    'reg_alpha': 0.877551,\n",
    "    'reg_lambda': 0.204082,\n",
    "    'silent': True,\n",
    "    'subsample': 0.949495,\n",
    "    'subsample_for_bin': 240000,\n",
    "    'subsample_freq': 1,\n",
    "    'metric': 'auc'\n",
    "    } \n",
    "        \n",
    "    train_data2 = lgb.Dataset(X_train,label=y_train2)\n",
    "    valid_data2 = lgb.Dataset(X_val,label = y_val2)\n",
    "    \n",
    "    #  Build Model\n",
    "    lgbm_clf = lgb.train(param2,\n",
    "    train_data2,\n",
    "    2500,\n",
    "    valid_sets=valid_data2,\n",
    "    early_stopping_rounds= 40,\n",
    "    verbose_eval= 100\n",
    "    )\n",
    "\n",
    "    #  Evaluate Model and Predict  \n",
    "    lgbclf  = lgbclf  + list(lgbm_clf.predict(X_val))\n",
    "    y_pred2 = lgbm_clf.predict(test)\n",
    "\n",
    "    #  feature importance\n",
    "    feature_importances_clf = pd.DataFrame({'feature': train.columns, 'importance': lgbm_clf.feature_importance()})    \n",
    "    zero_importance_features_clf = list(feature_importances_clf.feature[feature_importances_clf.importance == 0])\n",
    "\n",
    "    #  Add Predictions and Average Them\n",
    "    if i > 0:\n",
    "        fpred2 = pred2 + y_pred2\n",
    "        useless = list(set(useless) & set(zero_importance_features_clf) )\n",
    "    else:\n",
    "        fpred2 = y_pred2\n",
    "        useless = zero_importance_features_clf\n",
    "    pred2 = fpred2\n",
    "\n",
    "zero_importance_features_clf_union = useless\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對 Testing data 做預測，得出6個選項為前後文的機率後，各別乘上 $𝐶𝑜𝑟𝑟$ ，擇其最高作為6個選項中的預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_clf = lgbclf\n",
    "mpred2 = pred2 / folds\n",
    "train = train.drop(labels = zero_importance_features_clf_union, axis = 1)\n",
    "test = test.drop(labels = zero_importance_features_clf_union, axis = 1)\n",
    "\n",
    "train_pred = np.where(np.array(pred_clf) >= 0.5,1,0)\n",
    "test_pred = np.where(mpred2 >= 0.5,1,0)\n",
    "\n",
    "sum(train_pred == tauc)/len(train_pred)\n",
    "\n",
    "accuracy_score(train_pred, tauc)\n",
    "\n",
    "\n",
    "yy = mpred2.reshape(500,6)\n",
    "kk = (np.array(abs(test.iloc[:,400])) * mpred2).reshape(500,6)\n",
    "      \n",
    "prediction = np.argmax(kk,1)\n",
    "\n",
    "submit = pd.read_csv('Sample.csv')\n",
    "       \n",
    "submit['Answer'] =  prediction\n",
    "\n",
    "\n",
    "submit.to_csv('submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions \n",
    "\n",
    "+ Public leaderboard：0.708   \n",
    "+ Private leaderboard：0.656\n",
    "\n",
    "由於平常較少處理文字資料的經驗，這次從文字的前處理到後續的建模的每一步都非常具有挑戰性，但過程非常有趣，我們四個人也從中學到非常多。在比賽的前期我們把重心放在問句與回覆之間correlation的計算，有趣的是透過word2vec為基礎的向量計算我們就達到了60%左右的Accuracy，比亂猜的18%高出不少，從實作中發現了較相關的字詞向量之間的確會有較高的correlation。然而在後期我們透過建machine learning的模型時卻落入了public leaderboard的陷阱，最後選的submission都是在public leaderboard表現比較好的，而不是在Cross validation準確率比較高的模型設定，導致沒有選到private leaderboard表現最好的那個結果(0.68)。從這點我們得到了一個寶貴的經驗，那就是對待validation set的結果應該要像對待老婆那樣，老婆絕對不會有錯，如果有錯那也一定是我們看錯。最後，真的很感謝有這次很棒的實作機會，也希望在未來的四個比賽我們能夠有相對更好的結果！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
